<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Karaoke App (with Recorder)</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: linear-gradient(135deg,#667eea 0%,#764ba2 100%); min-height:100vh; padding:10px; }
        .container { max-width:100%; margin:0 auto; background:rgba(255,255,255,0.95); border-radius:15px; padding:15px; box-shadow:0 10px 30px rgba(0,0,0,0.3); }
        h1 { text-align:center; color:#667eea; margin-bottom:15px; font-size:1.5em; }
        .input-section { margin-bottom:15px; }
        .input-box { display:flex; flex-direction:column; gap:8px; }
        #videoInput { width:100%; padding:12px; font-size:14px; border:2px solid #667eea; border-radius:8px; outline:none; }
        #loadBtn { padding:12px; background:#667eea; color:white; border:none; border-radius:8px; cursor:pointer; font-size:15px; font-weight:bold; }
        .video-section { margin-bottom:15px; position:relative; padding-bottom:56.25%; height:0; overflow:hidden; }
        #youtubePlayer { position:absolute; top:0; left:0; width:100%; height:100%; border-radius:10px; }
        .mic-control { background:linear-gradient(135deg,#f5f7fa 0%,#c3cfe2 100%); padding:12px; border-radius:10px; margin-bottom:10px; }
        .mic-toggle-btn { width:100%; padding:15px; font-size:16px; background:#48bb78; margin-bottom:8px; }
        .mic-status { padding:8px; border-radius:8px; text-align:center; font-weight:bold; font-size:14px; }
        .mic-on { background:#48bb78; color:white; }
        .mic-off { background:#f56565; color:white; }
        .controls-grid { display:grid; grid-template-columns:1fr; gap:10px; }
        .control-panel { background:linear-gradient(135deg,#f5f7fa 0%,#c3cfe2 100%); padding:12px; border-radius:10px; }
        .panel-title { color:#667eea; margin-bottom:10px; font-size:1em; font-weight:bold; display:flex; align-items:center; gap:5px; }
        .control-row { margin-bottom:10px; }
        .control-label { display:flex; justify-content:space-between; margin-bottom:5px; font-size:13px; color:#333; }
        .label-text { font-weight:600; }
        .value-display { color:#667eea; font-weight:bold; }
        input[type="range"] { width:100%; height:6px; border-radius:3px; background:#ddd; outline:none; -webkit-appearance:none; }
        input[type="range"]::-webkit-slider-thumb { -webkit-appearance:none; width:18px; height:18px; border-radius:50%; background:#667eea; cursor:pointer; }
        input[type="range"]::-moz-range-thumb { width:18px; height:18px; border-radius:50%; background:#667eea; cursor:pointer; border:none; }
        .instructions { background:#fff3cd; padding:12px; border-radius:8px; margin-bottom:15px; border-left:3px solid #ffc107; font-size:13px; }
        .instructions h4 { color:#856404; margin-bottom:8px; font-size:14px; }
        .instructions ul { margin-left:18px; color:#856404; line-height:1.5; }
        .instructions li { margin-bottom:3px; }
        .row { display:flex; gap:8px; align-items:center; }
        .small-btn { padding:8px 12px; border-radius:6px; border:none; cursor:pointer; background:#667eea; color:white; font-weight:bold; }
        .danger { background:#f56565; }
        @media (min-width:768px) { .container { max-width:1000px; padding:25px; } h1 { font-size:2em; margin-bottom:20px; } .controls-grid { grid-template-columns:repeat(2,1fr); } }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé§ Karaoke App (v1 + Recorder)</h1>

        <div class="instructions">
            <h4>üìå H∆∞·ªõng d·∫´n:</h4>
            <ul>
                <li>D√°n link YouTube video karaoke v√† "T·∫£i Video".</li>
                <li>Nh·∫•n "B·∫≠t Micro" ƒë·ªÉ b·∫≠t micro (d√πng cho live playback/FX).</li>
                <li>ƒê·ªÉ ghi √¢m √¢m thanh web (tab) ho·∫∑c micro: ch·ªçn ngu·ªìn v√† "B·∫Øt ƒë·∫ßu ghi".</li>
                <li>Ghi √¢m tab y√™u c·∫ßu tr√¨nh duy·ªát h·ªó tr·ª£ getDisplayMedia audio (Chrome/Edge tr√™n desktop t·ªët nh·∫•t). iOS th∆∞·ªùng kh√¥ng cho ph√©p capture tab audio.</li>
                <li>Sau khi d·ª´ng ghi, nh·∫•n "T·∫£i WAV" ƒë·ªÉ l∆∞u file .wav</li>
            </ul>
        </div>

        <div class="input-section">
            <div class="input-box">
                <input type="text" id="videoInput" placeholder="D√°n link YouTube (VD: https://youtu.be/xxxxx ho·∫∑c https://www.youtube.com/watch?v=xxxxx)" />
                <button id="loadBtn">‚ñ∂Ô∏è T·∫£i Video</button>
            </div>
        </div>

        <div class="video-section">
            <div id="youtubePlayer"></div>
        </div>

        <div class="mic-control">
            <button class="mic-toggle-btn" id="micToggle">üé§ B·∫≠t Micro</button>
            <div class="mic-status mic-off" id="micStatus">Micro: T·∫ÆT</div>
        </div>

        <div class="controls-grid">
            <div class="control-panel">
                <div class="panel-title">üîä √Çm l∆∞·ª£ng & Ghi √¢m</div>

                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">YouTube (ch√≠nh th·ª©c)</span>
                        <span class="value-display" id="youtubeVolumeVal">10%</span>
                    </div>
                    <div class="row">
                        <input type="range" id="youtubeVolume" min="0" max="100" value="10" />
                        <button id="youtubeMuteBtn" class="small-btn" title="Mute/Unmute">Mute</button>
                    </div>
                </div>

                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">Ngu·ªìn ghi</span>
                        <span class="value-display" id="recordSourceVal">Mic</span>
                    </div>
                    <div class="row">
                        <select id="recordSource">
                            <option value="mic">Micro (getUserMedia)</option>
                            <option value="tab">Tab / System (getDisplayMedia)</option>
                            <option value="mix">Mic + Tab</option>
                        </select>
                    </div>
                </div>

                <div class="control-row">
                    <div class="row" style="width:100%;">
                        <button id="startRecordBtn" class="small-btn">üî¥ B·∫Øt ƒë·∫ßu ghi</button>
                        <button id="stopRecordBtn" class="small-btn danger" disabled>‚èπ D·ª´ng</button>
                        <button id="downloadWavBtn" class="small-btn" disabled>T·∫£i WAV</button>
                    </div>
                    <div style="font-size:12px;color:#444;margin-top:8px;">L∆∞u √Ω: file s·∫Ω ƒë∆∞·ª£c xu·∫•t d∆∞·ªõi d·∫°ng WAV (16-bit PCM) ch·ª©a √¢m thanh ƒë√£ ghi.</div>
                </div>

                <hr />

                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">Micro</span>
                        <span class="value-display" id="micVolumeVal">300%</span>
                    </div>
                    <input type="range" id="micVolume" min="0" max="400" value="300" />
                </div>

                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">Nh·∫°c (iOS: d√πng n√∫t tr√™n video)</span>
                        <span class="value-display" id="musicVolumeVal">10%</span>
                    </div>
                    <input type="range" id="musicVolume" min="0" max="100" value="10" />
                </div>
            </div>

            <div class="control-panel">
                <div class="panel-title">üéµ Echo</div>
                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">ƒê·ªô tr·ªÖ</span>
                        <span class="value-display" id="delayTimeVal">0.3s</span>
                    </div>
                    <input type="range" id="delayTime" min="0" max="1" step="0.05" value="0.3" />
                </div>
                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">Ph·∫£n h·ªìi</span>
                        <span class="value-display" id="feedbackVal">40%</span>
                    </div>
                    <input type="range" id="feedback" min="0" max="90" value="40" />
                </div>
                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">Mix</span>
                        <span class="value-display" id="wetVal">30%</span>
                    </div>
                    <input type="range" id="wet" min="0" max="100" value="30" />
                </div>
            </div>

            <div class="control-panel">
                <div class="panel-title">üéº Reverb</div>
                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">Th·ªùi gian</span>
                        <span class="value-display" id="reverbDecayVal">2.0s</span>
                    </div>
                    <input type="range" id="reverbDecay" min="0" max="10" step="0.1" value="2" />
                </div>
                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">Mix</span>
                        <span class="value-display" id="reverbWetVal">20%</span>
                    </div>
                    <input type="range" id="reverbWet" min="0" max="100" value="20" />
                </div>
            </div>

            <div class="control-panel">
                <div class="panel-title">üéöÔ∏è EQ</div>
                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">Bass</span>
                        <span class="value-display" id="bassVal">0 dB</span>
                    </div>
                    <input type="range" id="bass" min="-12" max="12" value="0" />
                </div>
                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">Mid</span>
                        <span class="value-display" id="midVal">0 dB</span>
                    </div>
                    <input type="range" id="mid" min="-12" max="12" value="0" />
                </div>
                <div class="control-row">
                    <div class="control-label">
                        <span class="label-text">Treble</span>
                        <span class="value-display" id="trebleVal">0 dB</span>
                    </div>
                    <input type="range" id="treble" min="-12" max="12" value="0" />
                </div>
            </div>
        </div>
    </div>

    <script src="https://www.youtube.com/iframe_api"></script>
    <script>
        // === Core vars from v1 (kept) ===
        let audioContext;
        let microphone; // used by micToggle flow (live FX)
        let micGainNode;
        let delayNode;
        let feedbackGainNode;
        let wetGainNode;
        let dryGainNode;
        let convolverNode;
        let reverbGainNode;
        let bassFilter;
        let midFilter;
        let trebleFilter;
        let isMicActive = false;
        let player;
        let playerReady = false;
        let masterGain; // central master output - good for tapping/recording

        // === Recorder-specific vars ===
        let recording = false;
        let recorderNode = null; // AudioWorkletNode or ScriptProcessorNode
        let recordedBuffers = []; // array per channel of Float32Array chunks
        let recordedLength = 0;
        let recSampleRate = 44100; // will be set from audioContext
        let recNumChannels = 2;
        let recSources = []; // { type: 'mic'|'tab', stream, sourceNode }
        let recStreamsToStop = []; // mediastreams to stop on end

        // YouTube player
        function onYouTubeIframeAPIReady() {
            player = new YT.Player('youtubePlayer', {
                height: '100%',
                width: '100%',
                videoId: '',
                playerVars: { 'autoplay': 0, 'controls': 1, 'rel': 0 },
                events: { 'onReady': onPlayerReady }
            });
        }
        function onPlayerReady(event) {
            playerReady = true;
            safeSetYouTubeVolume(10);
            // try mute/unmute to coax iOS
            try {
                player.mute();
                setTimeout(() => { player.unMute(); player.setVolume(10); }, 100);
            } catch (e) {}
        }

        // --- Helper: YouTube volume controls ---
        function safeSetYouTubeVolume(val) {
            if (!player || !player.setVolume) return;
            try {
                player.setVolume(val);
                document.getElementById('youtubeVolumeVal').textContent = val + '%';
                document.getElementById('musicVolume').value = val;
                document.getElementById('musicVolumeVal').textContent = val + '%';
                const muteBtn = document.getElementById('youtubeMuteBtn');
                muteBtn.textContent = (val === 0) ? 'Unmute' : 'Mute';
            } catch (e) {
                console.warn('setVolume failed (likely on iOS):', e);
            }
        }

        function extractVideoId(url) {
            const patterns = [
                /(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([^&\n?#]+)/,
                /^([a-zA-Z0-9_-]{11})$/
            ];
            for (let pattern of patterns) {
                const match = url.match(pattern);
                if (match) return match[1];
            }
            return null;
        }

        document.getElementById('loadBtn').addEventListener('click', loadVideo);
        document.getElementById('videoInput').addEventListener('keypress', function(e) { if (e.key === 'Enter') loadVideo(); });

        function loadVideo() {
            const input = document.getElementById('videoInput').value.trim();
            if (!input) { alert('Vui l√≤ng nh·∫≠p link YouTube!'); return; }
            const videoId = extractVideoId(input);
            if (videoId) {
                if (playerReady && player) {
                    player.loadVideoById(videoId);
                    player.playVideo();
                    setTimeout(() => { safeSetYouTubeVolume(parseInt(document.getElementById('youtubeVolume').value,10) || 10); }, 500);
                } else {
                    alert('Player ch∆∞a s·∫µn s√†ng, vui l√≤ng ƒë·ª£i v√†i gi√¢y v√† th·ª≠ l·∫°i!');
                }
            } else {
                alert('Link YouTube kh√¥ng h·ª£p l·ªá! Vui l√≤ng nh·∫≠p link ƒë√∫ng ƒë·ªãnh d·∫°ng.');
            }
        }

        // === AudioContext + FX init (based on v1) ===
        function initAudioContext() {
            if (audioContext) return;
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            recSampleRate = audioContext.sampleRate || 44100;

            // main processing nodes
            micGainNode = audioContext.createGain();
            delayNode = audioContext.createDelay(5.0);
            feedbackGainNode = audioContext.createGain();
            wetGainNode = audioContext.createGain();
            dryGainNode = audioContext.createGain();
            convolverNode = audioContext.createConvolver();
            reverbGainNode = audioContext.createGain();

            bassFilter = audioContext.createBiquadFilter();
            bassFilter.type = 'lowshelf';
            bassFilter.frequency.value = 200;

            midFilter = audioContext.createBiquadFilter();
            midFilter.type = 'peaking';
            midFilter.frequency.value = 1000;
            midFilter.Q.value = 0.5;

            trebleFilter = audioContext.createBiquadFilter();
            trebleFilter.type = 'highshelf';
            trebleFilter.frequency.value = 3000;

            // central masterGain so we can attach recorder easily
            masterGain = audioContext.createGain();
            masterGain.gain.value = 1.0;
            masterGain.connect(audioContext.destination);

            // default values
            micGainNode.gain.value = 3.0;
            delayNode.delayTime.value = 0.3;
            feedbackGainNode.gain.value = 0.4;
            wetGainNode.gain.value = 0.3;
            dryGainNode.gain.value = 1.0;
            reverbGainNode.gain.value = 0.2;

            createReverbImpulse(2.0);
        }

        function createReverbImpulse(duration) {
            if (!audioContext) return;
            const sampleRate = audioContext.sampleRate;
            const length = Math.floor(sampleRate * duration);
            const impulse = audioContext.createBuffer(2, length, sampleRate);
            const impulseL = impulse.getChannelData(0);
            const impulseR = impulse.getChannelData(1);
            for (let i = 0; i < length; i++) {
                const decay = Math.exp(-i / (sampleRate * duration * 0.5));
                impulseL[i] = (Math.random() * 2 - 1) * decay;
                impulseR[i] = (Math.random() * 2 - 1) * decay;
            }
            convolverNode.buffer = impulse;
        }

        // === Mic toggle (live FX) ===
        document.getElementById('micToggle').addEventListener('click', async function() {
            if (!isMicActive) {
                try {
                    initAudioContext();
                    if (audioContext.state === 'suspended') await audioContext.resume();

                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: { echoCancellation: false, noiseSuppression: false, autoGainControl: false, channelCount: { ideal: 2 } }
                    });

                    microphone = audioContext.createMediaStreamSource(stream);

                    // ensure stereo output even if mic is mono: splitter/merger trick
                    const settings = stream.getAudioTracks()[0].getSettings ? stream.getAudioTracks()[0].getSettings() : {};
                    const trackChannels = settings.channelCount || 1;
                    const splitter = audioContext.createChannelSplitter(Math.max(1, trackChannels));
                    const merger = audioContext.createChannelMerger(2);
                    microphone.connect(splitter);
                    if (trackChannels === 1) {
                        splitter.connect(merger, 0, 0);
                        splitter.connect(merger, 0, 1);
                    } else {
                        splitter.connect(merger, 0, 0);
                        splitter.connect(merger, 1, 1);
                    }

                    merger.connect(micGainNode);
                    micGainNode.connect(bassFilter);
                    bassFilter.connect(midFilter);
                    midFilter.connect(trebleFilter);

                    // connect FX outputs to masterGain (so both playback and recorder see same mixed output)
                    trebleFilter.connect(dryGainNode);
                    dryGainNode.connect(masterGain);

                    trebleFilter.connect(delayNode);
                    delayNode.connect(feedbackGainNode);
                    feedbackGainNode.connect(delayNode);
                    delayNode.connect(wetGainNode);
                    wetGainNode.connect(masterGain);

                    trebleFilter.connect(convolverNode);
                    convolverNode.connect(reverbGainNode);
                    reverbGainNode.connect(masterGain);

                    isMicActive = true;
                    this.textContent = 'üîá T·∫Øt Micro';
                    this.style.background = '#f56565';
                    document.getElementById('micStatus').textContent = 'Micro: B·∫¨T';
                    document.getElementById('micStatus').className = 'mic-status mic-on';
                } catch (error) {
                    console.error('L·ªói micro:', error);
                    alert('Kh√¥ng th·ªÉ truy c·∫≠p micro. Vui l√≤ng ki·ªÉm tra quy·ªÅn truy c·∫≠p!');
                }
            } else {
                try {
                    // Best-effort disconnect; we don't have direct references for splitter/merger here but
                    // disconnect main nodes that may be connected.
                    if (microphone) { try { microphone.disconnect(); } catch(e){} microphone = null; }
                    if (micGainNode) micGainNode.disconnect();
                    if (bassFilter) bassFilter.disconnect();
                    if (midFilter) midFilter.disconnect();
                    if (trebleFilter) trebleFilter.disconnect();
                    if (dryGainNode) dryGainNode.disconnect();
                    if (delayNode) delayNode.disconnect();
                    if (feedbackGainNode) feedbackGainNode.disconnect();
                    if (wetGainNode) wetGainNode.disconnect();
                    if (convolverNode) convolverNode.disconnect();
                    if (reverbGainNode) reverbGainNode.disconnect();
                } catch (e) {
                    console.warn('Error when disconnecting mic:', e);
                }

                isMicActive = false;
                this.textContent = 'üé§ B·∫≠t Micro';
                this.style.background = '#48bb78';
                document.getElementById('micStatus').textContent = 'Micro: T·∫ÆT';
                document.getElementById('micStatus').className = 'mic-status mic-off';
            }
        });

        // === FX controls ===
        document.getElementById('micVolume').addEventListener('input', function() {
            if (micGainNode) micGainNode.gain.value = this.value / 100;
            document.getElementById('micVolumeVal').textContent = this.value + '%';
        });
        document.getElementById('delayTime').addEventListener('input', function() {
            if (delayNode) delayNode.delayTime.value = parseFloat(this.value);
            document.getElementById('delayTimeVal').textContent = this.value + 's';
        });
        document.getElementById('feedback').addEventListener('input', function() {
            if (feedbackGainNode) feedbackGainNode.gain.value = this.value / 100;
            document.getElementById('feedbackVal').textContent = this.value + '%';
        });
        document.getElementById('wet').addEventListener('input', function() {
            if (wetGainNode) wetGainNode.gain.value = this.value / 100;
            document.getElementById('wetVal').textContent = this.value + '%';
        });
        document.getElementById('reverbDecay').addEventListener('input', function() {
            if (audioContext) createReverbImpulse(parseFloat(this.value));
            document.getElementById('reverbDecayVal').textContent = this.value + 's';
        });
        document.getElementById('reverbWet').addEventListener('input', function() {
            if (reverbGainNode) reverbGainNode.gain.value = this.value / 100;
            document.getElementById('reverbWetVal').textContent = this.value + '%';
        });
        document.getElementById('bass').addEventListener('input', function() {
            if (bassFilter) bassFilter.gain.value = parseFloat(this.value);
            document.getElementById('bassVal').textContent = this.value + ' dB';
        });
        document.getElementById('mid').addEventListener('input', function() {
            if (midFilter) midFilter.gain.value = parseFloat(this.value);
            document.getElementById('midVal').textContent = this.value + ' dB';
        });
        document.getElementById('treble').addEventListener('input', function() {
            if (trebleFilter) trebleFilter.gain.value = parseFloat(this.value);
            document.getElementById('trebleVal').textContent = this.value + ' dB';
        });

        // YouTube volume and mute
        document.getElementById('youtubeVolume').addEventListener('input', function() {
            const v = parseInt(this.value, 10);
            safeSetYouTubeVolume(v);
        });
        document.getElementById('youtubeMuteBtn').addEventListener('click', function() {
            if (!player) return;
            try {
                if (player.isMuted && player.isMuted()) {
                    player.unMute();
                    const v = parseInt(document.getElementById('youtubeVolume').value, 10) || 10;
                    player.setVolume(v);
                    this.textContent = 'Mute';
                } else {
                    player.mute();
                    this.textContent = 'Unmute';
                }
            } catch (e) {
                console.warn('Mute/unMute failed:', e);
            }
        });

        // Keep legacy musicVolume slider in sync
        document.getElementById('musicVolume').addEventListener('input', function() {
            const v = parseInt(this.value, 10);
            safeSetYouTubeVolume(v);
            document.getElementById('musicVolumeVal').textContent = this.value + '%';
        });

        // ========================
        // === Recording Logic  ===
        // ========================

        document.getElementById('recordSource').addEventListener('change', function() {
            document.getElementById('recordSourceVal').textContent = this.options[this.selectedIndex].text;
        });

        // Create recorder node: Try AudioWorklet first, fallback to ScriptProcessor
        async function createRecorderNode() {
            if (!audioContext) initAudioContext();
            // If already created, return
            if (recorderNode) return recorderNode;

            // prepare recordedBuffers
            recordedBuffers = [];
            recordedBuffers.length = recNumChannels;
            for (let i=0;i<recNumChannels;i++) recordedBuffers[i] = [];
            recordedLength = 0;

            // Try AudioWorklet
            if (audioContext.audioWorklet) {
                try {
                    const workletCode = `
                        class RecorderProcessor extends AudioWorkletProcessor {
                            constructor() {
                                super();
                            }
                            process (inputs, outputs) {
                                const input = inputs[0];
                                const output = outputs[0];
                                if (input && input.length > 0) {
                                    // copy input to output (pass-through)
                                    for (let ch = 0; ch < input.length; ch++) {
                                        output[ch].set(input[ch]);
                                    }
                                    // prepare transferable buffers
                                    const buffers = [];
                                    for (let ch = 0; ch < ${recNumChannels}; ch++) {
                                        // ensure channel exists
                                        const chanData = input[ch] || new Float32Array(input[0] ? input[0].length : 128).fill(0);
                                        const copy = new Float32Array(chanData);
                                        buffers.push(copy);
                                    }
                                    // transfer underlying ArrayBuffers to main thread for zero-copy
                                    this.port.postMessage({ samples: buffers }, buffers.map(b => b.buffer));
                                }
                                return true;
                            }
                        }
                        registerProcessor('recorder-processor', RecorderProcessor);
                    `;
                    const blob = new Blob([workletCode], { type: 'application/javascript' });
                    const url = URL.createObjectURL(blob);
                    await audioContext.audioWorklet.addModule(url);
                    const node = new AudioWorkletNode(audioContext, 'recorder-processor', { numberOfInputs: 1, numberOfOutputs: 1, outputChannelCount: [recNumChannels] });
                    node.port.onmessage = (e) => {
                        const { samples } = e.data;
                        // samples is array of Float32Array (transferred)
                        if (!samples) return;
                        const len = samples[0].length;
                        for (let ch=0; ch<recNumChannels; ch++) {
                            recordedBuffers[ch].push(samples[ch]);
                        }
                        recordedLength += len;
                    };
                    recorderNode = node;
                    return recorderNode;
                } catch (e) {
                    console.warn('AudioWorklet not available or failed, falling back to ScriptProcessor:', e);
                    recorderNode = null;
                }
            }

            // Fallback: ScriptProcessorNode (deprecated but works widely)
            try {
                const bufferSize = 4096;
                const scriptNode = audioContext.createScriptProcessor(bufferSize, recNumChannels, recNumChannels);
                scriptNode.onaudioprocess = function(evt) {
                    const inputBuffer = evt.inputBuffer;
                    const len = inputBuffer.length;
                    for (let ch = 0; ch < recNumChannels; ch++) {
                        const channelData = inputBuffer.getChannelData(ch);
                        recordedBuffers[ch].push(new Float32Array(channelData));
                    }
                    recordedLength += len;
                };
                recorderNode = scriptNode;
                return recorderNode;
            } catch (e) {
                console.error('Both AudioWorklet and ScriptProcessor creation failed:', e);
                throw e;
            }
        }

        // Start recording based on selected source
        document.getElementById('startRecordBtn').addEventListener('click', async function() {
            if (recording) return;
            initAudioContext();
            if (audioContext.state === 'suspended') await audioContext.resume();

            const source = document.getElementById('recordSource').value;
            recSources = [];
            recStreamsToStop = [];
            try {
                // Acquire streams as requested
                if (source === 'mic' || source === 'mix') {
                    const micStream = await navigator.mediaDevices.getUserMedia({ audio: { echoCancellation: false, noiseSuppression: false, autoGainControl: false } });
                    recSources.push({ type: 'mic', stream: micStream });
                    recStreamsToStop.push(micStream);
                }
                if (source === 'tab' || source === 'mix') {
                    // user will have to pick a tab/window/screen that provides audio
                    const dispStream = await navigator.mediaDevices.getDisplayMedia({ audio: true, video: false });
                    // Some browsers return a stream without audio even if requested; check
                    if (dispStream && dispStream.getAudioTracks().length > 0) {
                        recSources.push({ type: 'tab', stream: dispStream });
                        recStreamsToStop.push(dispStream);
                    } else {
                        // try alternative hint for Chrome
                        // (no reliable fallback; inform user)
                        alert('Kh√¥ng t√¨m th·∫•y audio t·ª´ tab/screen. M·ªôt s·ªë tr√¨nh duy·ªát kh√¥ng cho ph√©p capture audio ho·∫∑c b·∫°n ƒë√£ ch·ªçn ngu·ªìn kh√¥ng c√≥ √¢m thanh.');
                    }
                }

                if (recSources.length === 0) {
                    alert('Kh√¥ng c√≥ ngu·ªìn ghi ƒë∆∞·ª£c ch·ªçn/kh√¥ng th·ªÉ truy c·∫≠p ngu·ªìn.');
                    return;
                }

                // create recorder node
                const node = await createRecorderNode();

                // Create a mixing node for the recording graph
                const recGain = audioContext.createGain();
                recGain.gain.value = 1.0;

                // Connect recorder node in pass-through mode: recGain -> recorderNode -> (if node has output) audioContext.destination
                // For AudioWorkletNode created above, it has outputs, so we can connect it to masterGain to keep live monitoring; for ScriptProcessor, it processes ononaudio and passes audio automatically
                // We will route recGain -> recorderNode -> (if recorderNode.connect exists) -> audioContext.destination (mute-able)
                recGain.connect(node);
                // If recorderNode has outputs, connect to audioContext.destination for monitoring (you can comment this out to avoid monitoring)
                try {
                    node.connect(masterGain); // forward recorded audio into main master (so you can hear the captured mix)
                } catch (e) {
                    // some nodes (ScriptProcessor) may not accept connect in same way; ignore
                    try { node.connect(audioContext.destination); } catch (ee) {}
                }

                // For each source stream, create source node and connect to recGain
                for (const s of recSources) {
                    const srcNode = audioContext.createMediaStreamSource(s.stream);
                    // If source is mono, duplicate into stereo by linking to a channel merger
                    // But simplest: connect directly; AudioWorklet will handle channel mapping.
                    srcNode.connect(recGain);
                    s.sourceNode = srcNode;
                }

                recording = true;
                document.getElementById('startRecordBtn').disabled = true;
                document.getElementById('stopRecordBtn').disabled = false;
                document.getElementById('downloadWavBtn').disabled = true;
                console.log('Recording started. Sources:', recSources.map(r => r.type));

            } catch (err) {
                console.error('Start recording failed:', err);
                alert('B·∫Øt ƒë·∫ßu ghi th·∫•t b·∫°i: ' + (err.message || err));
            }
        });

        document.getElementById('stopRecordBtn').addEventListener('click', async function() {
            if (!recording) return;
            recording = false;
            // disconnect and stop source streams
            try {
                for (const s of recSources) {
                    try { if (s.sourceNode) s.sourceNode.disconnect(); } catch (e) {}
                }
                for (const ms of recStreamsToStop) {
                    try { ms.getTracks().forEach(t => t.stop()); } catch (e) {}
                }
                recSources = [];
                recStreamsToStop = [];
            } catch (e) {
                console.warn('Error cleaning rec sources:', e);
            }

            // Wait a tick to ensure collected buffers are flushed
            await new Promise(r => setTimeout(r, 100));

            document.getElementById('startRecordBtn').disabled = false;
            document.getElementById('stopRecordBtn').disabled = true;
            document.getElementById('downloadWavBtn').disabled = false;
            console.log('Recording stopped. Collected length:', recordedLength);
        });

        // WAV export (interleave and encode 16-bit PCM)
        function interleave(buffers, totalLen) {
            const result = new Float32Array(totalLen * recNumChannels);
            let offset = 0;
            const channelCount = recNumChannels;
            for (let i = 0; i < totalLen; i++) {
                for (let ch = 0; ch < channelCount; ch++) {
                    // not used here ‚Äî we will assemble from channel arrays
                }
            }
            return result;
        }

        function flattenChannel(chunks, length) {
            const result = new Float32Array(length);
            let offset = 0;
            for (let i = 0; i < chunks.length; i++) {
                result.set(chunks[i], offset);
                offset += chunks[i].length;
            }
            return result;
        }

        function encodeWAV(buffers, sampleRate) {
            const channelCount = recNumChannels;
            const samples = recordedLength;
            const bytesPerSample = 2;
            const blockAlign = channelCount * bytesPerSample;
            const buffer = new ArrayBuffer(44 + samples * blockAlign);
            const view = new DataView(buffer);

            /* RIFF identifier */
            writeString(view, 0, 'RIFF');
            /* file length */
            view.setUint32(4, 36 + samples * blockAlign, true);
            /* RIFF type */
            writeString(view, 8, 'WAVE');
            /* format chunk identifier */
            writeString(view, 12, 'fmt ');
            /* format chunk length */
            view.setUint32(16, 16, true);
            /* sample format (raw) */
            view.setUint16(20, 1, true);
            /* channel count */
            view.setUint16(22, channelCount, true);
            /* sample rate */
            view.setUint32(24, sampleRate, true);
            /* byte rate (sampleRate * blockAlign) */
            view.setUint32(28, sampleRate * blockAlign, true);
            /* block align (channelCount * bytesPerSample) */
            view.setUint16(32, blockAlign, true);
            /* bits per sample */
            view.setUint16(34, 8 * bytesPerSample, true);
            /* data chunk identifier */
            writeString(view, 36, 'data');
            /* data chunk length */
            view.setUint32(40, samples * blockAlign, true);

            // merge interleaved samples
            const channelData = [];
            for (let ch = 0; ch < channelCount; ch++) {
                channelData[ch] = flattenChannel(recordedBuffers[ch], recordedLength);
            }

            let offset = 44;
            // interleave and write 16-bit PCM
            for (let i = 0; i < recordedLength; i++) {
                for (let ch = 0; ch < channelCount; ch++) {
                    // clamp
                    let sample = channelData[ch][i];
                    if (isNaN(sample)) sample = 0;
                    sample = Math.max(-1, Math.min(1, sample));
                    const s = Math.floor(sample * 0x7fff);
                    view.setInt16(offset, s, true);
                    offset += 2;
                }
            }

            return new Blob([view], { type: 'audio/wav' });
        }

        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

        document.getElementById('downloadWavBtn').addEventListener('click', function() {
            if (recordedLength <= 0) { alert('Kh√¥ng c√≥ d·ªØ li·ªáu ghi √¢m.'); return; }
            const wavBlob = encodeWAV(recordedBuffers, recSampleRate);
            const url = URL.createObjectURL(wavBlob);
            const a = document.createElement('a');
            a.style.display = 'none';
            a.href = url;
            const now = new Date();
            const filename = `recording_${now.toISOString().replace(/[:.]/g,'-')}.wav`;
            a.download = filename;
            document.body.appendChild(a);
            a.click();
            setTimeout(() => {
                document.body.removeChild(a);
                URL.revokeObjectURL(url);
            }, 1000);
        });

        // Clean up on page unload
        window.addEventListener('beforeunload', function() {
            if (audioContext && audioContext.state !== 'closed') {
                try { audioContext.close(); } catch (e) {}
            }
        });

        // ==== End recording logic ====
    </script>
</body>
</html>
